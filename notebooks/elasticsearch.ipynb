{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from xml.etree import ElementTree as ET\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Almacenamieto de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "METADATA_FIELDS = [\n",
    "    (\"identificador\", None),\n",
    "    (\"titulo\", None),\n",
    "    (\"departamento\", None),\n",
    "    (\"fecha_publicacion\", lambda x: datetime.strptime(x, \"%Y%m%d\")),\n",
    "    (\"origen_legislativo\", None),\n",
    "    (\"rango\", None),\n",
    "]\n",
    "\n",
    "def jsonify_boe_entry(xml):\n",
    "    entry_json = {}\n",
    "\n",
    "    # get metadata\n",
    "    metadata = xml.find(\"metadatos\")\n",
    "    for tag, parser in METADATA_FIELDS:\n",
    "        element = metadata.find(tag)\n",
    "        if element is None: continue\n",
    "        text = element.text\n",
    "        entry_json[tag] = parser(text) if parser is not None else text\n",
    "    \n",
    "    # get topics\n",
    "    entry_json[\"materias\"] = [topic.text for topic in xml.findall(\".//materia\")]\n",
    "\n",
    "    # get references\n",
    "    past_refs = []\n",
    "    for ref in xml.findall(\".//anterior\"):\n",
    "        past_refs.append({\n",
    "            \"identificador\": ref.get(\"referencia\"),\n",
    "            \"texto\": ref.find(\"texto\").text,\n",
    "        })\n",
    "    entry_json[\"anteriores\"] = past_refs\n",
    "\n",
    "    future_refs = []\n",
    "    for ref in xml.findall(\".//posterior\"):\n",
    "        future_refs.append({\n",
    "            \"identificador\": ref.get(\"referencia\"),\n",
    "            \"texto\": ref.find(\"texto\").text,\n",
    "        })\n",
    "    entry_json[\"posteriores\"] = future_refs\n",
    "\n",
    "    # get XML text\n",
    "    xml_text = xml.find(\"texto\")\n",
    "    html_text = ET.tostring(xml_text, encoding='utf8', method=\"html\").decode('utf8')\n",
    "    html_text = \"\\n\".join(html_text.split(\"\\n\")[1:-1])\n",
    "    entry_json[\"texto\"] = html_text\n",
    "\n",
    "    # get paragraphs\n",
    "    entry_json[\"parrafos\"] = [paragraph.text for paragraph in xml_text.findall(\".//p\")]\n",
    "\n",
    "    return entry_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read all files in downloads folder, recursively\n",
    "def read_xml_files(path):\n",
    "    for r, _, f in os.walk(path):\n",
    "        for file in f:\n",
    "            if '.xml' not in file or \"BOE\" not in file:\n",
    "                continue\n",
    "            yield os.path.join(r, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data():\n",
    "    data = {}\n",
    "    i = 0\n",
    "    #itero por las carpetas de boe\n",
    "    for anio in os.listdir('boe/dias'):\n",
    "        #itero por los archivos de cada carpeta\n",
    "        for mes in os.listdir(f'boe/dias/{anio}'):\n",
    "            #si el archivo es un pdf\n",
    "            for dia in os.listdir(f'boe/dias/{anio}/{mes}'):\n",
    "                path_xmls = os.path.join('boe', 'dias', anio, mes, dia, 'xmls')\n",
    "                print(path_xmls)\n",
    "                for xml in os.listdir(path_xmls):\n",
    "                    path_xml = os.path.join(path_xmls, xml)\n",
    "                    xml = ET.parse(path_xml)\n",
    "                    entry_json = jsonify_boe_entry(xml)\n",
    "                    data[entry_json['identificador']] = entry_json\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(input_text, model, tokenizer, max_length=512):\n",
    "    enc = tokenizer(input_text, return_tensors='pt', truncation=True, max_length=max_length)\n",
    "    output = model.encoder(\n",
    "        input_ids=enc['input_ids'],\n",
    "        attention_mask=enc['attention_mask'],\n",
    "        return_dict=True\n",
    "        )\n",
    "    return output.last_hidden_state.tolist()[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ELASTICSEARCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install elasticsearch sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "import json\n",
    "\n",
    "#client = Elasticsearch(\"http://elasticsearch:9200\")\n",
    "client = Elasticsearch(\"http://localhost:9200\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crear index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"texto\": {\n",
    "                \"type\": \"text\"\n",
    "            },\n",
    "            \"semantic_embeddig\": {\n",
    "                \"type\": \"dense_vector\",\n",
    "                \"dims\": 512,\n",
    "                \"index\": True,\n",
    "                \"similarity\": \"cosine\"\n",
    "            },\n",
    "            #\"titulo\": {\"type\": \"text\"},\n",
    "            \"doc_id\": {\"type\": \"text\"},\n",
    "            #\"fecha_publicacion\": {\"type\": \"date\"}\n",
    "        }\n",
    "    },\n",
    "    \"settings\": {\n",
    "        \"analysis\": {\n",
    "            \"analyzer\": {\n",
    "                \"my_analyzer\": {\n",
    "                \"tokenizer\": \"keyword\",\n",
    "                \"char_filter\": [\n",
    "                    \"html_strip\"\n",
    "                ]\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"number_of_shards\": 2,\n",
    "        \"number_of_replicas\": 1\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Proyectos\\venv310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import T5Model, T5Tokenizer\n",
    "\n",
    "model = T5Model.from_pretrained('t5-small')\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index deleted\n"
     ]
    }
   ],
   "source": [
    "client = Elasticsearch(\"http://localhost:9200\")\n",
    "my_index = \"boe\"\n",
    "try:\n",
    "    client.indices.delete(index=my_index)\n",
    "    print(\"Index deleted\")\n",
    "except:\n",
    "    print(\"Index does not exist\")\n",
    "try:\n",
    "    client.indices.create(\n",
    "        index = my_index,\n",
    "        settings = config[\"settings\"],\n",
    "        mappings = config[\"mappings\"]\n",
    "    )\n",
    "except Exception as error:\n",
    "    print(\"Error:\", error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Poblar Ã­ndices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "boe\\dias\\2023\\10\\25\\xmls\n",
      "boe\\dias\\2023\\10\\26\\xmls\n",
      "boe\\dias\\2023\\10\\27\\xmls\n",
      "boe\\dias\\2023\\10\\28\\xmls\n",
      "boe\\dias\\2023\\10\\30\\xmls\n",
      "boe\\dias\\2023\\10\\31\\xmls\n",
      "boe\\dias\\2023\\11\\01\\xmls\n",
      "boe\\dias\\2023\\11\\02\\xmls\n",
      "boe\\dias\\2023\\11\\03\\xmls\n",
      "boe\\dias\\2023\\11\\04\\xmls\n",
      "boe\\dias\\2023\\11\\06\\xmls\n"
     ]
    }
   ],
   "source": [
    "data = process_data()\n",
    "#data[list(data.keys())[-5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOE-A-2023-21844\n"
     ]
    }
   ],
   "source": [
    "for key in list(data.keys())[:1]:\n",
    "    print(key)\n",
    "    fecha = data[key]['fecha_publicacion']\n",
    "    doc_id = key\n",
    "    full_text = data[key]['texto']\n",
    "    titulo = data[key]['titulo']\n",
    "    semantic_embeddigs = []\n",
    "    \n",
    "    for i, text in enumerate(data[key]['parrafos']):\n",
    "        semantic_embeddigs = get_embeddings(text, model, tokenizer)\n",
    "        document = {\n",
    "                \"doc_id\": doc_id,\n",
    "                \"embeddings\": semantic_embeddigs,\n",
    "                \"texto\": text,\n",
    "                #\"fecha_publicacion\": fecha,\n",
    "                #\"full_text\": full_text,\n",
    "                #\"titulo\": titulo\n",
    "            }\n",
    "\n",
    "        try:\n",
    "            client.index(\n",
    "                index = my_index,\n",
    "                document = document\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(f\"len: {len(document)}\")\n",
    "            print(fecha)\n",
    "            print(id)\n",
    "            print(document)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero de entradas: 100\n",
      "Entradas encontradas: dict_keys(['doc_id', 'embeddings', 'texto'])\n",
      "Nombre documento: BOE-A-2023-21844\n",
      "TamaÃ±o embeddings: 512\n",
      "Texto: El Real DecretoÂ 526/2014, deÂ 20 de junio, por el que se establece la lista de las enfermedades de los animales de declaraciÃ³n obligatoria y se regula su notificaciÃ³n, da cumplimiento a lo dispuesto en la DirectivaÂ 82/894/CEE del Consejo, deÂ 21 de diciembre deÂ 1982, relativa a la notificaciÃ³n de las enfermedades de los animales en la Comunidad, asÃ­ como a las obligaciones que el Reino de EspaÃ±a tiene como paÃ­s miembro de la OrganizaciÃ³n Mundial de Sanidad Animal (OMSA).\n"
     ]
    }
   ],
   "source": [
    "my_query = {\n",
    "    \"match_all\": {}\n",
    "}\n",
    "\n",
    "res = client.search(index=my_index, body={\"query\": my_query}, size=100)\n",
    "print(f\"Numero de entradas: {len(res['hits']['hits'])}\")\n",
    "print(f\"Entradas encontradas: {res['hits']['hits'][0]['_source'].keys()}\")\n",
    "print(f\"Nombre documento: {res['hits']['hits'][0]['_source']['doc_id']}\")\n",
    "print(f\"TamaÃ±o embeddings: {len(res['hits']['hits'][0]['_source']['embeddings'])}\")\n",
    "print(f\"Texto: {res['hits']['hits'][0]['_source']['texto']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BÃºsqueda lÃ©xica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOE-A-2023-21844\n",
      "Razones de seguridad jurÃ­dica, dado el alcance de las modificaciones a introducir en el Real DecretoÂ 526/2014, deÂ 20 de junio, aconsejan la aprobaciÃ³n de un nuevo real decreto, aunque la presente norma mantenga los elementos esenciales en cuanto a estructura y contenido del precedente, adecuÃ¡ndose al nuevo marco normativo antes expuesto.\n",
      "BOE-A-2023-21844\n",
      "Este proyecto se adecÃºa a los principios de buena regulaciÃ³n a que se refiere el artÃ­culoÂ 129 de la LeyÂ 39/2015, deÂ 1 de octubre, de Procedimiento Administrativo ComÃºn de las Administraciones PÃºblicas. En concreto, cumple con los principios de necesidad y eficacia, pues se articulan los instrumentos necesarios para la notificaciÃ³n de las enfermedades de declaraciÃ³n obligatoria, siendo una norma de carÃ¡cter bÃ¡sico con rango de real decreto que deroga la normativa en vigor con el fin de adaptar el mecanismo de notificaciÃ³n a lo dispuesto en la normativa europea, por lo que constituye el instrumento mÃ¡s eficaz para cumplir dicha obligaciÃ³n. Asimismo, se trata del instrumento mÃ¡s adecuado para garantizar que la normativa europea se aplica de un modo homogÃ©neo en todo el territorio nacional, lo que garantiza el interÃ©s general. TambiÃ©n se adecÃºa al principio de proporcionalidad, pues no existe otra alternativa menos restrictiva de derechos o que imponga menos obligaciones a los destinatarios. En cuanto a los principios de seguridad jurÃ­dica y eficiencia, esta norma se adecÃºa a los mismos y, en cuanto al principio de transparencia, se ha procurado la participaciÃ³n de las partes interesadas, a travÃ©s del procedimiento de informaciÃ³n y participaciÃ³n pÃºblica, evitando cargas administrativas.\n"
     ]
    }
   ],
   "source": [
    "string_to_search = \"seguridad jurÃ­dica\"\n",
    "lexic_query = {\n",
    "    \"fields\": [\"texto\"],\n",
    "    \"query\" : string_to_search,\n",
    "}\n",
    "my_query = {\n",
    "    \"simple_query_string\":{\n",
    "        \"fields\": [\"texto\"],\n",
    "        \"query\": string_to_search\n",
    "    }\n",
    "}\n",
    "res = client.search(index=my_index, query=my_query)\n",
    "for resp in res['hits']['hits']:\n",
    "    print(resp['_source']['doc_id'])\n",
    "    print(resp['_source']['texto'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BÃºsqueda semÃ¡ntica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Respuestas bÃºsqueda semÃ¡ntica:\n"
     ]
    }
   ],
   "source": [
    "string_to_search = \"DeclaraciÃ³n  de las enfermedades de los animales\"\n",
    "parametrs = {\n",
    "    \"field\": \"semantic_embeddig\",\n",
    "    \"query_vector\" : get_embeddings(string_to_search, model, tokenizer),\n",
    "    \"k\": 2,\n",
    "    \"num_candidates\": 5\n",
    "}\n",
    "res = client.search(index=my_index, knn=parametrs)\n",
    "print(\"Respuestas bÃºsqueda semÃ¡ntica:\")\n",
    "for resp in res['hits']['hits']:\n",
    "    print(resp['_source']['doc_id'])\n",
    "    print(resp['_source']['texto'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0rc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
