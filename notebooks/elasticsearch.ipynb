{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from xml.etree import ElementTree as ET\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Almacenamieto de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "METADATA_FIELDS = [\n",
    "    (\"identificador\", None),\n",
    "    (\"titulo\", None),\n",
    "    (\"departamento\", None),\n",
    "    (\"fecha_publicacion\", lambda x: datetime.strptime(x, \"%Y%m%d\")),\n",
    "    (\"origen_legislativo\", None),\n",
    "    (\"rango\", None),\n",
    "]\n",
    "\n",
    "def jsonify_boe_entry(xml):\n",
    "    entry_json = {}\n",
    "\n",
    "    # get metadata\n",
    "    metadata = xml.find(\"metadatos\")\n",
    "    for tag, parser in METADATA_FIELDS:\n",
    "        element = metadata.find(tag)\n",
    "        if element is None: continue\n",
    "        text = element.text\n",
    "        entry_json[tag] = parser(text) if parser is not None else text\n",
    "    \n",
    "    # get topics\n",
    "    entry_json[\"materias\"] = [topic.text for topic in xml.findall(\".//materia\")]\n",
    "\n",
    "    # get references\n",
    "    past_refs = []\n",
    "    for ref in xml.findall(\".//anterior\"):\n",
    "        past_refs.append({\n",
    "            \"identificador\": ref.get(\"referencia\"),\n",
    "            \"texto\": ref.find(\"texto\").text,\n",
    "        })\n",
    "    entry_json[\"anteriores\"] = past_refs\n",
    "\n",
    "    future_refs = []\n",
    "    for ref in xml.findall(\".//posterior\"):\n",
    "        future_refs.append({\n",
    "            \"identificador\": ref.get(\"referencia\"),\n",
    "            \"texto\": ref.find(\"texto\").text,\n",
    "        })\n",
    "    entry_json[\"posteriores\"] = future_refs\n",
    "\n",
    "    # get XML text\n",
    "    xml_text = xml.find(\"texto\")\n",
    "    html_text = ET.tostring(xml_text, encoding='utf8', method=\"html\").decode('utf8')\n",
    "    html_text = \"\\n\".join(html_text.split(\"\\n\")[1:-1])\n",
    "    entry_json[\"texto\"] = html_text\n",
    "\n",
    "    # get paragraphs\n",
    "    entry_json[\"parrafos\"] = [paragraph.text for paragraph in xml_text.findall(\".//p\")]\n",
    "\n",
    "    return entry_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read all files in downloads folder, recursively\n",
    "def read_xml_files(path):\n",
    "    for r, _, f in os.walk(path):\n",
    "        for file in f:\n",
    "            if '.xml' not in file or \"BOE\" not in file:\n",
    "                continue\n",
    "            yield os.path.join(r, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data():\n",
    "    data = {}\n",
    "    i = 0\n",
    "    #itero por las carpetas de boe\n",
    "    for anio in os.listdir('boe/dias'):\n",
    "        #itero por los archivos de cada carpeta\n",
    "        for mes in os.listdir(f'boe/dias/{anio}'):\n",
    "            #si el archivo es un pdf\n",
    "            for dia in os.listdir(f'boe/dias/{anio}/{mes}'):\n",
    "                path_xmls = os.path.join('boe', 'dias', anio, mes, dia, 'xmls')\n",
    "                print(path_xmls)\n",
    "                for xml in os.listdir(path_xmls):\n",
    "                    path_xml = os.path.join(path_xmls, xml)\n",
    "                    xml = ET.parse(path_xml)\n",
    "                    entry_json = jsonify_boe_entry(xml)\n",
    "                    data[entry_json['identificador']] = entry_json\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(input_text, model, tokenizer, max_length=512):\n",
    "    enc = tokenizer(input_text, return_tensors='pt', truncation=True, max_length=max_length)\n",
    "    output = model.encoder(\n",
    "        input_ids=enc['input_ids'],\n",
    "        attention_mask=enc['attention_mask'],\n",
    "        return_dict=True\n",
    "        )\n",
    "    return output.last_hidden_state.tolist()[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ELASTICSEARCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install elasticsearch sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "import json\n",
    "\n",
    "#client = Elasticsearch(\"http://elasticsearch:9200\")\n",
    "client = Elasticsearch(\"http://localhost:9200\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crear index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"texto\": {\n",
    "                \"type\": \"text\"\n",
    "            },\n",
    "            \"semantic_embeddig\": {\n",
    "                \"type\": \"dense_vector\",\n",
    "                \"dims\": 512,\n",
    "                \"index\": True,\n",
    "                \"similarity\": \"cosine\"\n",
    "            },\n",
    "            #\"titulo\": {\"type\": \"text\"},\n",
    "            \"doc_id\": {\"type\": \"text\"},\n",
    "            #\"fecha_publicacion\": {\"type\": \"date\"}\n",
    "        }\n",
    "    },\n",
    "    \"settings\": {\n",
    "        \"analysis\": {\n",
    "            \"analyzer\": {\n",
    "                \"my_analyzer\": {\n",
    "                \"tokenizer\": \"keyword\",\n",
    "                \"char_filter\": [\n",
    "                    \"html_strip\"\n",
    "                ]\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"number_of_shards\": 2,\n",
    "        \"number_of_replicas\": 1\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Proyectos\\venv310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import T5Model, T5Tokenizer\n",
    "\n",
    "model = T5Model.from_pretrained('t5-small')\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index deleted\n"
     ]
    }
   ],
   "source": [
    "client = Elasticsearch(\"http://localhost:9200\")\n",
    "my_index = \"boe\"\n",
    "try:\n",
    "    client.indices.delete(index=my_index)\n",
    "    print(\"Index deleted\")\n",
    "except:\n",
    "    print(\"Index does not exist\")\n",
    "try:\n",
    "    client.indices.create(\n",
    "        index = my_index,\n",
    "        settings = config[\"settings\"],\n",
    "        mappings = config[\"mappings\"]\n",
    "    )\n",
    "except Exception as error:\n",
    "    print(\"Error:\", error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Poblar índices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "boe\\dias\\2023\\10\\25\\xmls\n",
      "boe\\dias\\2023\\10\\26\\xmls\n",
      "boe\\dias\\2023\\10\\27\\xmls\n",
      "boe\\dias\\2023\\10\\28\\xmls\n",
      "boe\\dias\\2023\\10\\30\\xmls\n",
      "boe\\dias\\2023\\10\\31\\xmls\n",
      "boe\\dias\\2023\\11\\01\\xmls\n",
      "boe\\dias\\2023\\11\\02\\xmls\n",
      "boe\\dias\\2023\\11\\03\\xmls\n",
      "boe\\dias\\2023\\11\\04\\xmls\n",
      "boe\\dias\\2023\\11\\06\\xmls\n"
     ]
    }
   ],
   "source": [
    "data = process_data()\n",
    "#data[list(data.keys())[-5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOE-A-2023-21844\n"
     ]
    }
   ],
   "source": [
    "for key in list(data.keys())[:1]:\n",
    "    print(key)\n",
    "    fecha = data[key]['fecha_publicacion']\n",
    "    doc_id = key\n",
    "    full_text = data[key]['texto']\n",
    "    titulo = data[key]['titulo']\n",
    "    semantic_embeddigs = []\n",
    "    \n",
    "    for i, text in enumerate(data[key]['parrafos']):\n",
    "        semantic_embeddigs = get_embeddings(text, model, tokenizer)\n",
    "        document = {\n",
    "                \"doc_id\": doc_id,\n",
    "                \"embeddings\": semantic_embeddigs,\n",
    "                \"texto\": text,\n",
    "                #\"fecha_publicacion\": fecha,\n",
    "                #\"full_text\": full_text,\n",
    "                #\"titulo\": titulo\n",
    "            }\n",
    "\n",
    "        try:\n",
    "            client.index(\n",
    "                index = my_index,\n",
    "                document = document\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(f\"len: {len(document)}\")\n",
    "            print(fecha)\n",
    "            print(id)\n",
    "            print(document)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero de entradas: 100\n",
      "Entradas encontradas: dict_keys(['doc_id', 'embeddings', 'texto'])\n",
      "Nombre documento: BOE-A-2023-21844\n",
      "Tamaño embeddings: 512\n",
      "Texto: El Real Decreto 526/2014, de 20 de junio, por el que se establece la lista de las enfermedades de los animales de declaración obligatoria y se regula su notificación, da cumplimiento a lo dispuesto en la Directiva 82/894/CEE del Consejo, de 21 de diciembre de 1982, relativa a la notificación de las enfermedades de los animales en la Comunidad, así como a las obligaciones que el Reino de España tiene como país miembro de la Organización Mundial de Sanidad Animal (OMSA).\n"
     ]
    }
   ],
   "source": [
    "my_query = {\n",
    "    \"match_all\": {}\n",
    "}\n",
    "\n",
    "res = client.search(index=my_index, body={\"query\": my_query}, size=100)\n",
    "print(f\"Numero de entradas: {len(res['hits']['hits'])}\")\n",
    "print(f\"Entradas encontradas: {res['hits']['hits'][0]['_source'].keys()}\")\n",
    "print(f\"Nombre documento: {res['hits']['hits'][0]['_source']['doc_id']}\")\n",
    "print(f\"Tamaño embeddings: {len(res['hits']['hits'][0]['_source']['embeddings'])}\")\n",
    "print(f\"Texto: {res['hits']['hits'][0]['_source']['texto']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Búsqueda léxica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOE-A-2023-21844\n",
      "Razones de seguridad jurídica, dado el alcance de las modificaciones a introducir en el Real Decreto 526/2014, de 20 de junio, aconsejan la aprobación de un nuevo real decreto, aunque la presente norma mantenga los elementos esenciales en cuanto a estructura y contenido del precedente, adecuándose al nuevo marco normativo antes expuesto.\n",
      "BOE-A-2023-21844\n",
      "Este proyecto se adecúa a los principios de buena regulación a que se refiere el artículo 129 de la Ley 39/2015, de 1 de octubre, de Procedimiento Administrativo Común de las Administraciones Públicas. En concreto, cumple con los principios de necesidad y eficacia, pues se articulan los instrumentos necesarios para la notificación de las enfermedades de declaración obligatoria, siendo una norma de carácter básico con rango de real decreto que deroga la normativa en vigor con el fin de adaptar el mecanismo de notificación a lo dispuesto en la normativa europea, por lo que constituye el instrumento más eficaz para cumplir dicha obligación. Asimismo, se trata del instrumento más adecuado para garantizar que la normativa europea se aplica de un modo homogéneo en todo el territorio nacional, lo que garantiza el interés general. También se adecúa al principio de proporcionalidad, pues no existe otra alternativa menos restrictiva de derechos o que imponga menos obligaciones a los destinatarios. En cuanto a los principios de seguridad jurídica y eficiencia, esta norma se adecúa a los mismos y, en cuanto al principio de transparencia, se ha procurado la participación de las partes interesadas, a través del procedimiento de información y participación pública, evitando cargas administrativas.\n"
     ]
    }
   ],
   "source": [
    "string_to_search = \"seguridad jurídica\"\n",
    "lexic_query = {\n",
    "    \"fields\": [\"texto\"],\n",
    "    \"query\" : string_to_search,\n",
    "}\n",
    "my_query = {\n",
    "    \"simple_query_string\":{\n",
    "        \"fields\": [\"texto\"],\n",
    "        \"query\": string_to_search\n",
    "    }\n",
    "}\n",
    "res = client.search(index=my_index, query=my_query)\n",
    "for resp in res['hits']['hits']:\n",
    "    print(resp['_source']['doc_id'])\n",
    "    print(resp['_source']['texto'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Búsqueda semántica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Respuestas búsqueda semántica:\n"
     ]
    }
   ],
   "source": [
    "string_to_search = \"Declaración  de las enfermedades de los animales\"\n",
    "parametrs = {\n",
    "    \"field\": \"semantic_embeddig\",\n",
    "    \"query_vector\" : get_embeddings(string_to_search, model, tokenizer),\n",
    "    \"k\": 2,\n",
    "    \"num_candidates\": 5\n",
    "}\n",
    "res = client.search(index=my_index, knn=parametrs)\n",
    "print(\"Respuestas búsqueda semántica:\")\n",
    "for resp in res['hits']['hits']:\n",
    "    print(resp['_source']['doc_id'])\n",
    "    print(resp['_source']['texto'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0rc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
