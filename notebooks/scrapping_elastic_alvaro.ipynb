{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a233eb-f58b-48a3-beee-fcc9704fbbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "from xml.dom import minidom\n",
    "from xml.etree import ElementTree as ET\n",
    "import sys\n",
    "import threading\n",
    "import pprint\n",
    "\n",
    "BOE_URL = 'https://boe.es'\n",
    "BOE_API_SUMARIO = 'https://boe.es/diario_boe/xml.php?id=BOE-S-'\n",
    "START_DATE = datetime.strptime('20231025', '%Y%m%d')\n",
    "PATH_DATA = os.path.join('boe', 'dias')\n",
    "diff_1_day = timedelta(days=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b2094b-dbad-4c6d-8b43-9551cb981e99",
   "metadata": {},
   "source": [
    "# Scrappig BOE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a137da3-ac78-44bb-a490-2f16eca8fb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dirs(path):\n",
    "    try:\n",
    "        os.makedirs(path)\n",
    "    except FileExistsError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf0a30d-fba2-416e-ab64-9c2f54e49737",
   "metadata": {},
   "outputs": [],
   "source": [
    "def traer_documento(origen, destino):\n",
    "    intentos = 0\n",
    "    max_intentos = 5\n",
    "    \n",
    "    while intentos < max_intentos:\n",
    "        if intentos != 0:\n",
    "            import time\n",
    "            time.sleep(5)\n",
    "            print(f'Intento {intentos}')\n",
    "\n",
    "        response = requests.get(origen, stream=True)\n",
    "        if response.status_code == 200:\n",
    "            print(f'Guardando en: {destino}')\n",
    "            with open(destino, 'wb') as f:\n",
    "                for chunk in response.iter_content(1024):\n",
    "                    f.write(chunk)\n",
    "            break\n",
    "        else:\n",
    "            intentos += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b0bc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pdfs(current_date):\n",
    "    print('current_date:', current_date.strftime('%Y-%m-%d'))\n",
    "\n",
    "    fecha_anno, fecha_mes, fecha_dia = current_date.strftime('%Y %m %d').split()\n",
    "\n",
    "    for dir_path in [os.path.join(PATH_DATA, fecha_anno, fecha_mes, fecha_dia),\n",
    "                    os.path.join(PATH_DATA, fecha_anno, fecha_mes, fecha_dia, 'pdfs')]:\n",
    "        make_dirs(dir_path)\n",
    "\n",
    "    fichero_sumario_xml = os.path.join(PATH_DATA, fecha_anno, fecha_mes, fecha_dia, 'index.xml')\n",
    "\n",
    "    if os.path.exists(fichero_sumario_xml):\n",
    "        os.remove(fichero_sumario_xml)\n",
    "\n",
    "    print('Solicitando', f'{BOE_API_SUMARIO}{current_date.strftime(\"%Y%m%d\")} --> {fichero_sumario_xml}')\n",
    "    traer_documento(f'{BOE_API_SUMARIO}{current_date.strftime(\"%Y%m%d\")}', fichero_sumario_xml)\n",
    "\n",
    "    tamano_sumario_xml = os.path.getsize(fichero_sumario_xml)\n",
    "    print('Recibidos:', tamano_sumario_xml, 'bytes')\n",
    "\n",
    "    if tamano_sumario_xml < 10:\n",
    "        print('ERROR: Sumario XML erróneo o incompleto')\n",
    "        sys.exit(1)\n",
    "\n",
    "    xml_sumario = minidom.parse(fichero_sumario_xml)\n",
    "\n",
    "    if xml_sumario.documentElement.nodeName == 'error':\n",
    "        os.remove(fichero_sumario_xml)\n",
    "        os.rmdir(os.path.join(PATH_DATA, fecha_anno, fecha_mes, fecha_dia, 'pdfs'))\n",
    "        os.rmdir(os.path.join(PATH_DATA, fecha_anno, fecha_mes, fecha_dia))\n",
    "        print('AVISO: No existen boletines para la current_date', current_date.strftime('%Y-%m-%d'))\n",
    "    else:\n",
    "        pdfs = xml_sumario.getElementsByTagName('urlPdf')\n",
    "        print(f\"{len(pdfs)} encontrados\")\n",
    "        for pdf in pdfs:\n",
    "            fichero_pdf = os.path.join(PATH_DATA, fecha_anno, fecha_mes, fecha_dia, 'pdfs', pdf.firstChild.nodeValue).replace(' ', '_').replace('/', '\\\\')[1:]\n",
    "            fichero_pdf_tamano_xml = pdf.getAttribute('szBytes')\n",
    "            if os.path.exists(fichero_pdf):\n",
    "                print(\"fichero encontrado\")\n",
    "                if os.path.getsize(fichero_pdf) == int(fichero_pdf_tamano_xml):\n",
    "                    continue\n",
    "                else:\n",
    "                    try:\n",
    "                        os.remove(fichero_pdf)\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "\n",
    "            print('Solicitando', f'{BOE_URL}{pdf.firstChild.nodeValue} --> {fichero_pdf}')\n",
    "            intentos = 0\n",
    "            max_intentos = 5\n",
    "            while intentos < max_intentos:\n",
    "                if intentos != 0:\n",
    "                    import time\n",
    "                    time.sleep(5)\n",
    "                    print(f'Intento {intentos}')\n",
    "\n",
    "                traer_documento(f'{BOE_URL}{pdf.firstChild.nodeValue}', fichero_pdf)\n",
    "                intentos += 1\n",
    "\n",
    "                if os.path.getsize(fichero_pdf) == int(fichero_pdf_tamano_xml):\n",
    "                    break\n",
    "\n",
    "            if os.path.getsize(fichero_pdf) != int(fichero_pdf_tamano_xml):\n",
    "                print('ERROR: El tamaño del fichero PDF descargado no coincide con el del XML del Sumario '\n",
    "                    f'(Descargado: {os.getsize(fichero_pdf)} <> XML: {fichero_pdf_tamano_xml})')\n",
    "                sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ce0ebc-f9b4-4c82-8ce8-595aa43958dc",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_xmls(current_date, semaphore=None):\n",
    "    print('current_date:', current_date.strftime('%Y-%m-%d'))\n",
    "\n",
    "    fecha_anno, fecha_mes, fecha_dia = current_date.strftime('%Y %m %d').split()\n",
    "\n",
    "    for dir_path in [os.path.join(PATH_DATA, fecha_anno, fecha_mes, fecha_dia),\n",
    "                    os.path.join(PATH_DATA, fecha_anno, fecha_mes, fecha_dia, 'xmls')]:\n",
    "        make_dirs(dir_path)\n",
    "\n",
    "    fichero_sumario_xml = os.path.join(PATH_DATA, fecha_anno, fecha_mes, fecha_dia, 'index.xml')\n",
    "\n",
    "    if os.path.exists(fichero_sumario_xml):\n",
    "        os.remove(fichero_sumario_xml)\n",
    "\n",
    "    print('Solicitando', f'{BOE_API_SUMARIO}{current_date.strftime(\"%Y%m%d\")} --> {fichero_sumario_xml}')\n",
    "    traer_documento(f'{BOE_API_SUMARIO}{current_date.strftime(\"%Y%m%d\")}', fichero_sumario_xml)\n",
    "\n",
    "    tamano_sumario_xml = os.path.getsize(fichero_sumario_xml)\n",
    "    print('Recibidos:', tamano_sumario_xml, 'bytes')\n",
    "\n",
    "    if tamano_sumario_xml < 10:\n",
    "        print('ERROR: Sumario XML erróneo o incompleto')\n",
    "        sys.exit(1)\n",
    "\n",
    "    xml_sumario = minidom.parse(fichero_sumario_xml)\n",
    "\n",
    "    if xml_sumario.documentElement.nodeName == 'error':\n",
    "        os.remove(fichero_sumario_xml)\n",
    "        os.rmdir(os.path.join(PATH_DATA, fecha_anno, fecha_mes, fecha_dia, 'xmls'))\n",
    "        os.rmdir(os.path.join(PATH_DATA, fecha_anno, fecha_mes, fecha_dia))\n",
    "        print('AVISO: No existen boletines para la fecha ', current_date.strftime('%Y-%m-%d'))\n",
    "    else:\n",
    "        xmls = xml_sumario.getElementsByTagName('urlXml')\n",
    "        for xml in xmls:\n",
    "            fichero_xml = PATH_DATA + '\\\\' + str(fecha_anno) + '\\\\' + str(fecha_mes) + '\\\\' + str(fecha_dia) + '\\\\' + 'xmls' + '\\\\' + xml.firstChild.nodeValue.split('=')[-1] + '.xml'\n",
    "            #fichero_xml = os.path.join(PATH_DATA, fecha_anno, fecha_mes, fecha_dia, 'xmls', xml.firstChild.nodeValue.split('=')[-1] + '.xml').replace(' ', '_').replace('/', '\\\\')[1:]\n",
    "\n",
    "            if os.path.exists(fichero_xml):\n",
    "                print(\"fichero encontrado\")\n",
    "                continue\n",
    "\n",
    "\n",
    "            print('Solicitando', f'{BOE_URL}{xml.firstChild.nodeValue} --> {fichero_xml}')\n",
    "            intentos = 0\n",
    "            max_intentos = 5\n",
    "            while intentos < max_intentos:\n",
    "                if intentos != 0:\n",
    "                    import time\n",
    "                    time.sleep(5)\n",
    "                    print(f'Intento {intentos}')\n",
    "\n",
    "                traer_documento(f'{BOE_URL}{xml.firstChild.nodeValue}', fichero_xml)\n",
    "                intentos += 1\n",
    "\n",
    "                if os.path.getsize(fichero_xml):\n",
    "                    break\n",
    "\n",
    "            if not os.path.getsize(fichero_xml):\n",
    "                print('ERROR: El tamaño del fichero xml descargado no coincide con el del XML del Sumario '\n",
    "                    f'(Descargado: {os.getsize(fichero_xml)} <> XML: {fichero_xml_tamano_xml})')\n",
    "                sys.exit(1)\n",
    "\n",
    "    if semaphore is not None:\n",
    "        semaphore.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0411924-87d2-442f-8ae8-1d276388311b",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_date = START_DATE\n",
    "hoy = datetime.now()\n",
    "while current_date <= hoy and 0:\n",
    "    get_xmls(current_date)\n",
    "    get_pdfs(current_date)\n",
    "    current_date += diff_1_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18abfa03",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hilos = 6\n",
    "semaphore = threading.Semaphore(n_hilos)\n",
    "dates =  [START_DATE + timedelta(days=i) for i in range((datetime.now() - START_DATE).days + 1)]\n",
    "hilos = []\n",
    "\n",
    "print(dates)\n",
    "\n",
    "for date in dates:\n",
    "    semaphore.acquire()\n",
    "\n",
    "    print('current_date:', date.strftime('%Y-%m-%d'))\n",
    "    \n",
    "    hilo = threading.Thread(target=get_xmls, args=(date, semaphore))\n",
    "    #threading.Thread(target=get_pdfs, args=(date,)).start()\n",
    "    hilos.append(hilo)\n",
    "    hilo.start()\n",
    "\n",
    "for hilo in hilos:\n",
    "    hilo.join()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0736864",
   "metadata": {},
   "source": [
    "# Almacenamieto de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663dce3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def proceso_sumario(path_xml, keys_to_save=None):\n",
    "    data = {}\n",
    "    xml_sumario = minidom.parse(path_xml)\n",
    "\n",
    "    anno, mes, dia = path_xml.split('\\\\')[2:-2]\n",
    "    file_name = path_xml.split('\\\\')[-1].split('.')[0]\n",
    "    date = path_xml.split('\\\\')[-1].split('.')[0].split('-')[-2]\n",
    "\n",
    "    data['index'] = file_name.replace(date, f'{anno}{mes}{dia}')\n",
    "    data['index'] = path_xml\n",
    "    \n",
    "    documento = xml_sumario.documentElement\n",
    "    i = 0\n",
    "    for element in documento.getElementsByTagName('*'):\n",
    "        #print(\"Elemento:\", element.tagName)\n",
    "        if element.tagName == 'p': element.tagName = element.getAttribute('class') + '_' + str(i); i += 1\n",
    "        data[element.tagName] = {}\n",
    "        for attr_name, attr_value in element.attributes.items():\n",
    "            data[element.tagName][attr_name] = attr_value\n",
    "            #print(f\"Atributo: {attr_name} - Valor: {attr_value}\")\n",
    "        if element.firstChild and element.firstChild.nodeType == element.TEXT_NODE:\n",
    "            data[element.tagName]['Contenido'] = element.firstChild.data\n",
    "            #print(\"Contenido:\", element.firstChild.data)\n",
    "\n",
    "    data_to_save = {}\n",
    "    if keys_to_save is None:\n",
    "        keys_to_save = data.keys()\n",
    "    else:\n",
    "        keys_to_save.extend([e for i, e in enumerate(data.keys()) if e.startswith('parrafo') or e.startswith('articulo')])\n",
    "        \n",
    "    for key in keys_to_save:\n",
    "        try:\n",
    "            if key == 'index':\n",
    "                data_to_save[key] = data[key]\n",
    "            else:\n",
    "                data_to_save[key] = data[key]['Contenido'].replace('\\xa0', ' ').replace('\\u2003', ' ')\n",
    "        except Exception as e:\n",
    "            pass\n",
    "            #print(f\"{path_xml} excepcion: {e}\")\n",
    "    \n",
    "    return data_to_save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data_old(keys_to_save=None):\n",
    "    data = {}\n",
    "    i = 0\n",
    "    #itero por las carpetas de boe\n",
    "    for anio in os.listdir('boe/dias'):\n",
    "        #itero por los archivos de cada carpeta\n",
    "        for mes in os.listdir(f'boe/dias/{anio}'):\n",
    "            #si el archivo es un pdf\n",
    "            for dia in os.listdir(f'boe/dias/{anio}/{mes}'):\n",
    "                path_xmls = os.path.join('boe', 'dias', anio, mes, dia, 'xmls')\n",
    "                print(path_xmls)\n",
    "                for xml in os.listdir(path_xmls):\n",
    "                    path_xml = os.path.join(path_xmls, xml)\n",
    "                    file_data = proceso_sumario(path_xml, keys_to_save)\n",
    "                    data[file_data['identificador']] = file_data\n",
    "                    i += 1\n",
    "                    #if i > 10: break\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7a5b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "METADATA_FIELDS = [\n",
    "    (\"identificador\", None),\n",
    "    (\"titulo\", None),\n",
    "    (\"departamento\", None),\n",
    "    (\"fecha_publicacion\", lambda x: datetime.strptime(x, \"%Y%m%d\")),\n",
    "    (\"origen_legislativo\", None),\n",
    "    (\"rango\", None),\n",
    "]\n",
    "\n",
    "def jsonify_boe_entry(xml):\n",
    "    entry_json = {}\n",
    "\n",
    "    # get metadata\n",
    "    metadata = xml.find(\"metadatos\")\n",
    "    for tag, parser in METADATA_FIELDS:\n",
    "        element = metadata.find(tag)\n",
    "        if element is None: continue\n",
    "        text = element.text\n",
    "        entry_json[tag] = parser(text) if parser is not None else text\n",
    "    \n",
    "    # get topics\n",
    "    entry_json[\"materias\"] = [topic.text for topic in xml.findall(\".//materia\")]\n",
    "\n",
    "    # get references\n",
    "    past_refs = []\n",
    "    for ref in xml.findall(\".//anterior\"):\n",
    "        past_refs.append({\n",
    "            \"identificador\": ref.get(\"referencia\"),\n",
    "            \"texto\": ref.find(\"texto\").text,\n",
    "        })\n",
    "    entry_json[\"anteriores\"] = past_refs\n",
    "\n",
    "    future_refs = []\n",
    "    for ref in xml.findall(\".//posterior\"):\n",
    "        future_refs.append({\n",
    "            \"identificador\": ref.get(\"referencia\"),\n",
    "            \"texto\": ref.find(\"texto\").text,\n",
    "        })\n",
    "    entry_json[\"posteriores\"] = future_refs\n",
    "\n",
    "    # get XML text\n",
    "    xml_text = xml.find(\"texto\")\n",
    "    html_text = ET.tostring(xml_text, encoding='utf8', method=\"html\").decode('utf8')\n",
    "    html_text = \"\\n\".join(html_text.split(\"\\n\")[1:-1])\n",
    "    entry_json[\"texto\"] = html_text\n",
    "\n",
    "    # get paragraphs\n",
    "    entry_json[\"parrafos\"] = [paragraph.text for paragraph in xml_text.findall(\".//p\")]\n",
    "\n",
    "    return entry_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e2cd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read all files in downloads folder, recursively\n",
    "def read_xml_files(path):\n",
    "    for r, _, f in os.walk(path):\n",
    "        for file in f:\n",
    "            if '.xml' not in file or \"BOE\" not in file:\n",
    "                continue\n",
    "            yield os.path.join(r, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5a8153",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data():\n",
    "    data = {}\n",
    "    i = 0\n",
    "    #itero por las carpetas de boe\n",
    "    for anio in os.listdir('boe/dias'):\n",
    "        #itero por los archivos de cada carpeta\n",
    "        for mes in os.listdir(f'boe/dias/{anio}'):\n",
    "            #si el archivo es un pdf\n",
    "            for dia in os.listdir(f'boe/dias/{anio}/{mes}'):\n",
    "                path_xmls = os.path.join('boe', 'dias', anio, mes, dia, 'xmls')\n",
    "                print(path_xmls)\n",
    "                for xml in os.listdir(path_xmls):\n",
    "                    path_xml = os.path.join(path_xmls, xml)\n",
    "                    xml = ET.parse(path_xml)\n",
    "                    entry_json = jsonify_boe_entry(xml)\n",
    "                    data[entry_json['identificador']] = entry_json\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c08fb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(input_text, model, tokenizer, max_length=512):\n",
    "    enc = tokenizer(input_text, return_tensors='pt', truncation=True, max_length=max_length)\n",
    "    output = model.encoder(\n",
    "        input_ids=enc['input_ids'],\n",
    "        attention_mask=enc['attention_mask'],\n",
    "        return_dict=True\n",
    "        )\n",
    "    return output.last_hidden_state.tolist()[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee1eb87",
   "metadata": {},
   "source": [
    "## ELASTICSEARCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df1ae83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install elasticsearch sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abefc678",
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "import json\n",
    "\n",
    "#client = Elasticsearch(\"http://elasticsearch:9200\")\n",
    "client = Elasticsearch(\"http://localhost:9200\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b0f7f5",
   "metadata": {},
   "source": [
    "### Crear index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db051f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"texto\": {\n",
    "                \"type\": \"text\"\n",
    "            },\n",
    "            \"semantic_embeddig\": {\n",
    "                \"type\": \"dense_vector\",\n",
    "                \"dims\": 512,\n",
    "                \"index\": True,\n",
    "                \"similarity\": \"cosine\"\n",
    "            },\n",
    "            #\"titulo\": {\"type\": \"text\"},\n",
    "            \"doc_id\": {\"type\": \"text\"},\n",
    "            #\"fecha_publicacion\": {\"type\": \"date\"}\n",
    "        }\n",
    "    },\n",
    "    \"settings\": {\n",
    "        \"analysis\": {\n",
    "            \"analyzer\": {\n",
    "                \"my_analyzer\": {\n",
    "                \"tokenizer\": \"keyword\",\n",
    "                \"char_filter\": [\n",
    "                    \"html_strip\"\n",
    "                ]\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"number_of_shards\": 2,\n",
    "        \"number_of_replicas\": 1\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf02fd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import T5Model, T5Tokenizer\n",
    "\n",
    "model = T5Model.from_pretrained('t5-small')\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b60b672",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Elasticsearch(\"http://localhost:9200\")\n",
    "my_index = \"boe\"\n",
    "try:\n",
    "    client.indices.delete(index=my_index)\n",
    "    print(\"Index deleted\")\n",
    "except:\n",
    "    print(\"Index does not exist\")\n",
    "try:\n",
    "    client.indices.create(\n",
    "        index = my_index,\n",
    "        settings = config[\"settings\"],\n",
    "        mappings = config[\"mappings\"]\n",
    "    )\n",
    "except Exception as error:\n",
    "    print(\"Error:\", error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d891357e",
   "metadata": {},
   "source": [
    "### Poblar índices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2bb0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = process_data()\n",
    "data[list(data.keys())[-5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in list(data.keys())[:1]:\n",
    "    print(key)\n",
    "    fecha = data[key]['fecha_publicacion']\n",
    "    doc_id = key\n",
    "    full_text = data[key]['texto']\n",
    "    titulo = data[key]['titulo']\n",
    "    semantic_embeddigs = []\n",
    "    \n",
    "    for i, text in enumerate(data[key]['parrafos']):\n",
    "        semantic_embeddigs = get_embeddings(text, model, tokenizer)\n",
    "        document = {\n",
    "                \"doc_id\": doc_id,\n",
    "                \"embeddings\": semantic_embeddigs,\n",
    "                \"texto\": text,\n",
    "                #\"fecha_publicacion\": fecha,\n",
    "                #\"full_text\": full_text,\n",
    "                #\"titulo\": titulo\n",
    "            }\n",
    "\n",
    "        try:\n",
    "            client.index(\n",
    "                index = my_index,\n",
    "                document = document\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(f\"len: {len(document)}\")\n",
    "            print(fecha)\n",
    "            print(id)\n",
    "            print(document)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_query = {\n",
    "    \"match_all\": {}\n",
    "}\n",
    "\n",
    "res = client.search(index=my_index, body={\"query\": my_query}, size=100)\n",
    "print(f\"Numero de entradas: {len(res['hits']['hits'])}\")\n",
    "print(f\"Entradas encontradas: {res['hits']['hits'][0]['_source'].keys()}\")\n",
    "print(f\"Nombre documento: {res['hits']['hits'][0]['_source']['doc_id']}\")\n",
    "print(f\"Tamaño embeddings: {len(res['hits']['hits'][0]['_source']['embeddings'])}\")\n",
    "print(f\"Texto: {res['hits']['hits'][0]['_source']['texto']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d27f7d",
   "metadata": {},
   "source": [
    "### Búsqueda léxica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82dc0bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "string_to_search = \"seguridad jurídica\"\n",
    "lexic_query = {\n",
    "    \"fields\": [\"texto\"],\n",
    "    \"query\" : string_to_search,\n",
    "}\n",
    "my_query = {\n",
    "    \"simple_query_string\":{\n",
    "        \"fields\": [\"texto\"],\n",
    "        \"query\": string_to_search\n",
    "    }\n",
    "}\n",
    "res = client.search(index=my_index, query=my_query)\n",
    "for resp in res['hits']['hits']:\n",
    "    print(resp['_source']['doc_id'])\n",
    "    print(resp['_source']['texto'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2b0da2",
   "metadata": {},
   "source": [
    "### Búsqueda semántica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f8e9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "string_to_search = \"Declaración  de las enfermedades de los animales\"\n",
    "parametrs = {\n",
    "    \"field\": \"semantic_embeddig\",\n",
    "    \"query_vector\" : get_embeddings(string_to_search, model, tokenizer),\n",
    "    \"k\": 2,\n",
    "    \"num_candidates\": 5\n",
    "}\n",
    "res = client.search(index=my_index, knn=parametrs)\n",
    "print(\"Respuestas búsqueda semántica:\")\n",
    "for resp in res['hits']['hits']:\n",
    "    print(resp['_source']['doc_id'])\n",
    "    print(resp['_source']['texto'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c727be",
   "metadata": {},
   "source": [
    "## MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b17eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pymongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7217f1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139e089e",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient(\"mongodb://root:example@mongo:27017\")\n",
    "db = client.BOE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = process_data()\n",
    "data[list(data.keys())[-5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in data.keys():\n",
    "    db.Wikipedia_NER.update_one({'path':data[key]['index']},{\"$set\": data[key]}, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7aa3941",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
