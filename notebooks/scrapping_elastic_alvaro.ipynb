{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a233eb-f58b-48a3-beee-fcc9704fbbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "from xml.dom import minidom\n",
    "import sys\n",
    "import threading\n",
    "import pprint\n",
    "\n",
    "BOE_URL = 'https://boe.es'\n",
    "BOE_API_SUMARIO = 'https://boe.es/diario_boe/xml.php?id=BOE-S-'\n",
    "START_DATE = datetime.strptime('20231001', '%Y%m%d')\n",
    "PATH_DATA = os.path.join('boe', 'dias')\n",
    "diff_1_day = timedelta(days=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b2094b-dbad-4c6d-8b43-9551cb981e99",
   "metadata": {},
   "source": [
    "# Scrappig BOE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e5e67a",
   "metadata": {},
   "source": [
    "## Funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a137da3-ac78-44bb-a490-2f16eca8fb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dirs(path):\n",
    "    try:\n",
    "        os.makedirs(path)\n",
    "    except FileExistsError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf0a30d-fba2-416e-ab64-9c2f54e49737",
   "metadata": {},
   "outputs": [],
   "source": [
    "def traer_documento(origen, destino):\n",
    "    intentos = 0\n",
    "    max_intentos = 5\n",
    "    \n",
    "    while intentos < max_intentos:\n",
    "        if intentos != 0:\n",
    "            import time\n",
    "            time.sleep(5)\n",
    "            print(f'Intento {intentos}')\n",
    "\n",
    "        response = requests.get(origen, stream=True)\n",
    "        if response.status_code == 200:\n",
    "            print(f'Guardando en: {destino}')\n",
    "            with open(destino, 'wb') as f:\n",
    "                for chunk in response.iter_content(1024):\n",
    "                    f.write(chunk)\n",
    "            break\n",
    "        else:\n",
    "            intentos += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b0bc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pdfs(current_date):\n",
    "    print('current_date:', current_date.strftime('%Y-%m-%d'))\n",
    "\n",
    "    fecha_anno, fecha_mes, fecha_dia = current_date.strftime('%Y %m %d').split()\n",
    "\n",
    "    for dir_path in [os.path.join(PATH_DATA, fecha_anno, fecha_mes, fecha_dia),\n",
    "                    os.path.join(PATH_DATA, fecha_anno, fecha_mes, fecha_dia, 'pdfs')]:\n",
    "        make_dirs(dir_path)\n",
    "\n",
    "    fichero_sumario_xml = os.path.join(PATH_DATA, fecha_anno, fecha_mes, fecha_dia, 'index.xml')\n",
    "\n",
    "    if os.path.exists(fichero_sumario_xml):\n",
    "        os.remove(fichero_sumario_xml)\n",
    "\n",
    "    print('Solicitando', f'{BOE_API_SUMARIO}{current_date.strftime(\"%Y%m%d\")} --> {fichero_sumario_xml}')\n",
    "    traer_documento(f'{BOE_API_SUMARIO}{current_date.strftime(\"%Y%m%d\")}', fichero_sumario_xml)\n",
    "\n",
    "    tamano_sumario_xml = os.path.getsize(fichero_sumario_xml)\n",
    "    print('Recibidos:', tamano_sumario_xml, 'bytes')\n",
    "\n",
    "    if tamano_sumario_xml < 10:\n",
    "        print('ERROR: Sumario XML err칩neo o incompleto')\n",
    "        sys.exit(1)\n",
    "\n",
    "    xml_sumario = minidom.parse(fichero_sumario_xml)\n",
    "\n",
    "    if xml_sumario.documentElement.nodeName == 'error':\n",
    "        os.remove(fichero_sumario_xml)\n",
    "        os.rmdir(os.path.join(PATH_DATA, fecha_anno, fecha_mes, fecha_dia, 'pdfs'))\n",
    "        os.rmdir(os.path.join(PATH_DATA, fecha_anno, fecha_mes, fecha_dia))\n",
    "        print('AVISO: No existen boletines para la current_date', current_date.strftime('%Y-%m-%d'))\n",
    "    else:\n",
    "        pdfs = xml_sumario.getElementsByTagName('urlPdf')\n",
    "        print(f\"{len(pdfs)} encontrados\")\n",
    "        for pdf in pdfs:\n",
    "            fichero_pdf = os.path.join(PATH_DATA, fecha_anno, fecha_mes, fecha_dia, 'pdfs', pdf.firstChild.nodeValue).replace(' ', '_').replace('/', '\\\\')[1:]\n",
    "            fichero_pdf_tamano_xml = pdf.getAttribute('szBytes')\n",
    "            if os.path.exists(fichero_pdf):\n",
    "                print(\"fichero encontrado\")\n",
    "                if os.path.getsize(fichero_pdf) == int(fichero_pdf_tamano_xml):\n",
    "                    continue\n",
    "                else:\n",
    "                    try:\n",
    "                        os.remove(fichero_pdf)\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "\n",
    "            print('Solicitando', f'{BOE_URL}{pdf.firstChild.nodeValue} --> {fichero_pdf}')\n",
    "            intentos = 0\n",
    "            max_intentos = 5\n",
    "            while intentos < max_intentos:\n",
    "                if intentos != 0:\n",
    "                    import time\n",
    "                    time.sleep(5)\n",
    "                    print(f'Intento {intentos}')\n",
    "\n",
    "                traer_documento(f'{BOE_URL}{pdf.firstChild.nodeValue}', fichero_pdf)\n",
    "                intentos += 1\n",
    "\n",
    "                if os.path.getsize(fichero_pdf) == int(fichero_pdf_tamano_xml):\n",
    "                    break\n",
    "\n",
    "            if os.path.getsize(fichero_pdf) != int(fichero_pdf_tamano_xml):\n",
    "                print('ERROR: El tama침o del fichero PDF descargado no coincide con el del XML del Sumario '\n",
    "                    f'(Descargado: {os.getsize(fichero_pdf)} <> XML: {fichero_pdf_tamano_xml})')\n",
    "                sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ce0ebc-f9b4-4c82-8ce8-595aa43958dc",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_xmls(current_date, semaphore=None):\n",
    "    print('current_date:', current_date.strftime('%Y-%m-%d'))\n",
    "\n",
    "    fecha_anno, fecha_mes, fecha_dia = current_date.strftime('%Y %m %d').split()\n",
    "\n",
    "    for dir_path in [os.path.join(PATH_DATA, fecha_anno, fecha_mes, fecha_dia),\n",
    "                    os.path.join(PATH_DATA, fecha_anno, fecha_mes, fecha_dia, 'xmls')]:\n",
    "        make_dirs(dir_path)\n",
    "\n",
    "    fichero_sumario_xml = os.path.join(PATH_DATA, fecha_anno, fecha_mes, fecha_dia, 'index.xml')\n",
    "\n",
    "    if os.path.exists(fichero_sumario_xml):\n",
    "        os.remove(fichero_sumario_xml)\n",
    "\n",
    "    print('Solicitando', f'{BOE_API_SUMARIO}{current_date.strftime(\"%Y%m%d\")} --> {fichero_sumario_xml}')\n",
    "    traer_documento(f'{BOE_API_SUMARIO}{current_date.strftime(\"%Y%m%d\")}', fichero_sumario_xml)\n",
    "\n",
    "    tamano_sumario_xml = os.path.getsize(fichero_sumario_xml)\n",
    "    print('Recibidos:', tamano_sumario_xml, 'bytes')\n",
    "\n",
    "    if tamano_sumario_xml < 10:\n",
    "        print('ERROR: Sumario XML err칩neo o incompleto')\n",
    "        sys.exit(1)\n",
    "\n",
    "    xml_sumario = minidom.parse(fichero_sumario_xml)\n",
    "\n",
    "    if xml_sumario.documentElement.nodeName == 'error':\n",
    "        os.remove(fichero_sumario_xml)\n",
    "        os.rmdir(os.path.join(PATH_DATA, fecha_anno, fecha_mes, fecha_dia, 'xmls'))\n",
    "        os.rmdir(os.path.join(PATH_DATA, fecha_anno, fecha_mes, fecha_dia))\n",
    "        print('AVISO: No existen boletines para la fecha ', current_date.strftime('%Y-%m-%d'))\n",
    "    else:\n",
    "        xmls = xml_sumario.getElementsByTagName('urlXml')\n",
    "        for xml in xmls:\n",
    "            fichero_xml = PATH_DATA + '\\\\' + str(fecha_anno) + '\\\\' + str(fecha_mes) + '\\\\' + str(fecha_dia) + '\\\\' + 'xmls' + '\\\\' + xml.firstChild.nodeValue.split('=')[-1] + '.xml'\n",
    "            #fichero_xml = os.path.join(PATH_DATA, fecha_anno, fecha_mes, fecha_dia, 'xmls', xml.firstChild.nodeValue.split('=')[-1] + '.xml').replace(' ', '_').replace('/', '\\\\')[1:]\n",
    "\n",
    "            if os.path.exists(fichero_xml):\n",
    "                print(\"fichero encontrado\")\n",
    "                continue\n",
    "\n",
    "\n",
    "            print('Solicitando', f'{BOE_URL}{xml.firstChild.nodeValue} --> {fichero_xml}')\n",
    "            intentos = 0\n",
    "            max_intentos = 5\n",
    "            while intentos < max_intentos:\n",
    "                if intentos != 0:\n",
    "                    import time\n",
    "                    time.sleep(5)\n",
    "                    print(f'Intento {intentos}')\n",
    "\n",
    "                traer_documento(f'{BOE_URL}{xml.firstChild.nodeValue}', fichero_xml)\n",
    "                intentos += 1\n",
    "\n",
    "                if os.path.getsize(fichero_xml):\n",
    "                    break\n",
    "\n",
    "            if not os.path.getsize(fichero_xml):\n",
    "                print('ERROR: El tama침o del fichero xml descargado no coincide con el del XML del Sumario '\n",
    "                    f'(Descargado: {os.getsize(fichero_xml)} <> XML: {fichero_xml_tamano_xml})')\n",
    "                sys.exit(1)\n",
    "\n",
    "    if semaphore is not None:\n",
    "        semaphore.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0411924-87d2-442f-8ae8-1d276388311b",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_date = START_DATE\n",
    "hoy = datetime.now()\n",
    "while current_date <= hoy and 0:\n",
    "    get_xmls(current_date)\n",
    "    get_pdfs(current_date)\n",
    "    current_date += diff_1_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18abfa03",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hilos = 6\n",
    "semaphore = threading.Semaphore(n_hilos)\n",
    "dates =  [START_DATE + timedelta(days=i) for i in range((datetime.now() - START_DATE).days + 1)]\n",
    "hilos = []\n",
    "\n",
    "print(dates)\n",
    "\n",
    "for date in dates:\n",
    "    semaphore.acquire()\n",
    "\n",
    "    print('current_date:', date.strftime('%Y-%m-%d'))\n",
    "    \n",
    "    hilo = threading.Thread(target=get_xmls, args=(date, semaphore))\n",
    "    #threading.Thread(target=get_pdfs, args=(date,)).start()\n",
    "    hilos.append(hilo)\n",
    "    hilo.start()\n",
    "\n",
    "for hilo in hilos:\n",
    "    hilo.join()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0736864",
   "metadata": {},
   "source": [
    "# Almacenamieto de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663dce3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def proceso_sumario(path_xml):\n",
    "    data = {}\n",
    "    xml_sumario = minidom.parse(path_xml)\n",
    "\n",
    "    anno, mes, dia = path_xml.split('\\\\')[2:-2]\n",
    "    file_name = path_xml.split('\\\\')[-1].split('.')[0]\n",
    "    date = path_xml.split('\\\\')[-1].split('.')[0].split('-')[-2]\n",
    "\n",
    "    data['index'] = file_name.replace(date, f'{anno}{mes}{dia}')\n",
    "\n",
    "    aux = {}\n",
    "    documento = xml_sumario.documentElement\n",
    "    i = 0\n",
    "    for element in documento.getElementsByTagName('*'):\n",
    "        #print(\"Elemento:\", element.tagName)\n",
    "        if element.tagName == 'p': element.tagName = element.getAttribute('class') + '_' + str(i); i += 1\n",
    "        data[element.tagName] = {}\n",
    "        for attr_name, attr_value in element.attributes.items():\n",
    "            data[element.tagName][attr_name] = attr_value\n",
    "            #print(f\"Atributo: {attr_name} - Valor: {attr_value}\")\n",
    "        if element.firstChild and element.firstChild.nodeType == element.TEXT_NODE:\n",
    "            data[element.tagName]['Contenido'] = element.firstChild.data\n",
    "            #print(\"Contenido:\", element.firstChild.data)\n",
    "\n",
    "    data_to_save = {}\n",
    "    keys_to_save = ['identificador', 'index', 'origen_legislativo', 'departamento', 'titulo', 'fecha_publicacion']\n",
    "    #keys_to_save = keys_to_save.extend([i for i in data.keys() if i.startswith('parrafo')])\n",
    "    keys_to_save.extend([e for i, e in enumerate(data.keys()) if e.startswith('parrafo') or e.startswith('articulo')])\n",
    "    for key in keys_to_save:\n",
    "        try:\n",
    "            if key == 'index':\n",
    "                data_to_save[key] = data[key]\n",
    "            elif key.startswith('parrafo'):\n",
    "                data_to_save[key] = data[key]['Contenido'].replace('\\xa0', ' ').replace('\\u2003', ' ')\n",
    "            else:\n",
    "                data_to_save[key] = data[key]['Contenido'].replace('\\xa0', ' ').replace('\\u2003', ' ')\n",
    "        except Exception as e:\n",
    "            pass\n",
    "            #print(f\"{path_xml} excepcion: {e}\")\n",
    "    \n",
    "    return data_to_save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data():\n",
    "    data = {}\n",
    "    i = 0\n",
    "    #itero por las carpetas de boe\n",
    "    for anio in os.listdir('boe/dias'):\n",
    "        #itero por los archivos de cada carpeta\n",
    "        for mes in os.listdir(f'boe/dias/{anio}'):\n",
    "            #si el archivo es un pdf\n",
    "            for dia in os.listdir(f'boe/dias/{anio}/{mes}'):\n",
    "                path_xmls = os.path.join('boe', 'dias', anio, mes, dia, 'xmls')\n",
    "                print(path_xmls)\n",
    "                for xml in os.listdir(path_xmls):\n",
    "                    path_xml = os.path.join(path_xmls, xml)\n",
    "                    file_data = proceso_sumario(path_xml)\n",
    "                    data[file_data['identificador']] = file_data\n",
    "                    i += 1\n",
    "                    #if i > 10: break\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c08fb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(input_text, model, tokenizer, max_length=512):\n",
    "    enc = tokenizer(input_text, return_tensors='pt', add_special_tokens=True)\n",
    "    output = model.encoder(\n",
    "        input_ids=enc['input_ids'],\n",
    "        attention_mask=enc['attention_mask'],\n",
    "        return_dict=True\n",
    "        )\n",
    "    return output.last_hidden_state.tolist()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2bb0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = process_data()\n",
    "data['BOE-A-2023-20397']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df1ae83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install elasticsearch sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abefc678",
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "import json\n",
    "\n",
    "#client = Elasticsearch(\"http://elasticsearch:9200\")\n",
    "client = Elasticsearch(\"http://localhost:9200\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b0f7f5",
   "metadata": {},
   "source": [
    "## Crear index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62048fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "           \"text\": {\"type\": \"text\", \"similarity\": \"BM25\"},\n",
    "            \"embeddings\": {\n",
    "                \"type\": \"dense_vector\",\n",
    "                \"dims\": 512,\n",
    "                \"index\": True,\n",
    "                \"similarity\": \"cosine\"\n",
    "            },\n",
    "            \"doc_id\": {\"type\": \"keyword\"},\n",
    "            \"fecha_publicacion\": {\"type\": \"date\"}\n",
    "        }\n",
    "    },\n",
    "    \"settings\": {\n",
    "        \"number_of_shards\": 2,\n",
    "        \"number_of_replicas\": 1\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf02fd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import T5Model, T5Tokenizer\n",
    "\n",
    "model = T5Model.from_pretrained('t5-small')\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b60b672",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_index = \"boe\"\n",
    "try:\n",
    "    client.indices.create(\n",
    "        index = my_index,\n",
    "        settings = config[\"settings\"],\n",
    "        mappings = config[\"mappings\"]\n",
    "    )\n",
    "except Exception as error:\n",
    "    print(\"Error:\", error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in data.keys():\n",
    "    print(key)\n",
    "    fecha = datetime.strptime(data[key]['fecha_publicacion'], '%Y%m%d')\n",
    "    id = key\n",
    "    document = data[key]\n",
    "\n",
    "    keys_to_save = [key for key in document.keys() if key.startswith('parrafo') or key.startswith('articulo')]\n",
    "    for key_to_save in keys_to_save:\n",
    "        try:\n",
    "            client.index(\n",
    "                index = my_index,\n",
    "                document = {\n",
    "                    \"fecha_publicacion\": fecha,\n",
    "                    \"doc_id\": document['identificador'],\n",
    "                    \"text\": document[key_to_save],\n",
    "                    \"embeddings\": get_embeddings(document[key_to_save], model, tokenizer)\n",
    "                },\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(f\"len: {len(document)}\")\n",
    "            print(fecha)\n",
    "            print(id)\n",
    "            print(document)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_query = {\n",
    "    \"match_all\": {}\n",
    "}\n",
    "\n",
    "res = client.search(index=my_index, body={\"query\": my_query}, size=100)\n",
    "pprint.pprint(res['hits'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7aa3941",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0rc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
